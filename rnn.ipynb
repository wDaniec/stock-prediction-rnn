{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"eurpln_d.csv\"\n",
    "BATCH_SIZE = 4\n",
    "TRAIN_TEST_RATIO = 0.8\n",
    "NUM_EPOCHS = 40\n",
    "LR_RATE = 0.0001\n",
    "IN_SIZE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(FILE_PATH)\n",
    "data = data.drop([\"Data\",\"Otwarcie\",\"Najwyzszy\",\"Najnizszy\"], axis=1)\n",
    "data.dropna(axis=0,inplace=True)\n",
    "data = torch.Tensor(data.to_numpy())\n",
    "\n",
    "data_min = torch.min(data, dim=0, keepdim=True).values\n",
    "data_max = torch.max(data, dim=0, keepdim=True).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, shift_size, data_min, data_max):\n",
    "        self.data = data\n",
    "        self.labels = self.data\n",
    "        self.shift_size = shift_size\n",
    "        self.data = (self.data - data_min)/(data_max-data_min)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0] - self.shift_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx += self.shift_size\n",
    "        x = self.data[(idx-self.shift_size):idx].t()\n",
    "        y = self.labels[idx].unsqueeze(1)\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = int(TRAIN_TEST_RATIO*len(data))\n",
    "train_data = data[:idx]\n",
    "test_data = data[idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = MyDataset(train_data, IN_SIZE, data_min, data_max)\n",
    "dataset_test = MyDataset(test_data, IN_SIZE, data_min, data_max)\n",
    "trainloader = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=False)\n",
    "testloader = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(trainloader))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__ (self,input_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size,16)\n",
    "        self.fc2 = nn.Linear(16,8)\n",
    "        self.fc3 = nn.Linear(8,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(input_size=IN_SIZE)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "train_loss: 14.565702438354492\n",
      "test_loss: 13.751509666442871\n",
      "epoch: 1\n",
      "train_loss: 12.961514472961426\n",
      "test_loss: 11.860633850097656\n",
      "epoch: 2\n",
      "train_loss: 11.082265853881836\n",
      "test_loss: 9.61493968963623\n",
      "epoch: 3\n",
      "train_loss: 8.872008323669434\n",
      "test_loss: 7.064929485321045\n",
      "epoch: 4\n",
      "train_loss: 6.456127166748047\n",
      "test_loss: 4.490236282348633\n",
      "epoch: 5\n",
      "train_loss: 4.1398468017578125\n",
      "test_loss: 2.3239939212799072\n",
      "epoch: 6\n",
      "train_loss: 2.292426347732544\n",
      "test_loss: 0.9247596263885498\n",
      "epoch: 7\n",
      "train_loss: 1.13287353515625\n",
      "test_loss: 0.31722190976142883\n",
      "epoch: 8\n",
      "train_loss: 0.5865617394447327\n",
      "test_loss: 0.19273848831653595\n",
      "epoch: 9\n",
      "train_loss: 0.39235085248947144\n",
      "test_loss: 0.21879900991916656\n",
      "epoch: 10\n",
      "train_loss: 0.33201754093170166\n",
      "test_loss: 0.24674877524375916\n",
      "epoch: 11\n",
      "train_loss: 0.30855241417884827\n",
      "test_loss: 0.2538841962814331\n",
      "epoch: 12\n",
      "train_loss: 0.29376253485679626\n",
      "test_loss: 0.24870407581329346\n",
      "epoch: 13\n",
      "train_loss: 0.2810157239437103\n",
      "test_loss: 0.2385445386171341\n",
      "epoch: 14\n",
      "train_loss: 0.2687346935272217\n",
      "test_loss: 0.22673092782497406\n",
      "epoch: 15\n",
      "train_loss: 0.2565065920352936\n",
      "test_loss: 0.2144925445318222\n",
      "epoch: 16\n",
      "train_loss: 0.2442367523908615\n",
      "test_loss: 0.20224961638450623\n",
      "epoch: 17\n",
      "train_loss: 0.23193000257015228\n",
      "test_loss: 0.19014757871627808\n",
      "epoch: 18\n",
      "train_loss: 0.21962140500545502\n",
      "test_loss: 0.1782459169626236\n",
      "epoch: 19\n",
      "train_loss: 0.2073541283607483\n",
      "test_loss: 0.16657857596874237\n",
      "epoch: 20\n",
      "train_loss: 0.19517281651496887\n",
      "test_loss: 0.15517421066761017\n",
      "epoch: 21\n",
      "train_loss: 0.1831195056438446\n",
      "test_loss: 0.14405657351016998\n",
      "epoch: 22\n",
      "train_loss: 0.1712341010570526\n",
      "test_loss: 0.1332489401102066\n",
      "epoch: 23\n",
      "train_loss: 0.1595553308725357\n",
      "test_loss: 0.12277457863092422\n",
      "epoch: 24\n",
      "train_loss: 0.14811916649341583\n",
      "test_loss: 0.11265597492456436\n",
      "epoch: 25\n",
      "train_loss: 0.13695985078811646\n",
      "test_loss: 0.10291364043951035\n",
      "epoch: 26\n",
      "train_loss: 0.12610948085784912\n",
      "test_loss: 0.09356874227523804\n",
      "epoch: 27\n",
      "train_loss: 0.11559909582138062\n",
      "test_loss: 0.08464041352272034\n",
      "epoch: 28\n",
      "train_loss: 0.10545743256807327\n",
      "test_loss: 0.07614750415086746\n",
      "epoch: 29\n",
      "train_loss: 0.09571167826652527\n",
      "test_loss: 0.06810708343982697\n",
      "epoch: 30\n",
      "train_loss: 0.08638805150985718\n",
      "test_loss: 0.06053462624549866\n",
      "epoch: 31\n",
      "train_loss: 0.07751002907752991\n",
      "test_loss: 0.05344375967979431\n",
      "epoch: 32\n",
      "train_loss: 0.06909973919391632\n",
      "test_loss: 0.046845149248838425\n",
      "epoch: 33\n",
      "train_loss: 0.06117692589759827\n",
      "test_loss: 0.04074717313051224\n",
      "epoch: 34\n",
      "train_loss: 0.05375857651233673\n",
      "test_loss: 0.035154078155756\n",
      "epoch: 35\n",
      "train_loss: 0.04685917869210243\n",
      "test_loss: 0.030066348612308502\n",
      "epoch: 36\n",
      "train_loss: 0.04048994183540344\n",
      "test_loss: 0.025480376556515694\n",
      "epoch: 37\n",
      "train_loss: 0.034657832235097885\n",
      "test_loss: 0.021387306973338127\n",
      "epoch: 38\n",
      "train_loss: 0.029365859925746918\n",
      "test_loss: 0.017773274332284927\n",
      "epoch: 39\n",
      "train_loss: 0.024611838161945343\n",
      "test_loss: 0.014619249850511551\n"
     ]
    }
   ],
   "source": [
    "#learning the model\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(\"epoch: {}\".format(epoch))\n",
    "    train_loss = 0.0\n",
    "    for x, y in trainloader:\n",
    "        #zero the parameters gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #forward, backward, optimize\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y)\n",
    "        train_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"train_loss: {}\".format(train_loss / len(trainloader)))\n",
    "    \n",
    "    \n",
    "    test_loss = 0.0\n",
    "    model.eval()\n",
    "    for x, y in testloader:\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y)\n",
    "        test_loss += loss\n",
    "    print(\"test_loss: {}\".format(test_loss / len(testloader)))\n",
    "    model.train()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.6157, 0.5877, 0.6540]],\n",
      "\n",
      "        [[0.5877, 0.6540, 0.6526]],\n",
      "\n",
      "        [[0.6540, 0.6526, 0.7049]],\n",
      "\n",
      "        [[0.6526, 0.7049, 0.6842]]])\n",
      "tensor([[[4.3072]],\n",
      "\n",
      "        [[4.3210]],\n",
      "\n",
      "        [[4.3155]],\n",
      "\n",
      "        [[4.3202]]])\n",
      "tensor([[[4.2970]],\n",
      "\n",
      "        [[4.3082]],\n",
      "\n",
      "        [[4.3371]],\n",
      "\n",
      "        [[4.3626]]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x,y = next(iter(testloader))\n",
    "print(x)\n",
    "print(y)\n",
    "print(model(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# making predictions\n",
    "predictions = []\n",
    "net.eval()\n",
    "for i, data in enumerate(test):\n",
    "    \n",
    "    inputs, labels = data[1:4], data[0]\n",
    "\n",
    "    predicted = net(inputs)\n",
    "    predictions.append(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_min_max[1][0][0])\n",
    "print(predictions[:10])\n",
    "test_min_max[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#najpierw \"odskaluje\" dane\n",
    "predictions = torch.Tensor(predictions)\n",
    "print(predictions[:10])\n",
    "predictions = predictions*(test_min_max[1][0][0]-test_min_max[0][0][0])+test_min_max[0][0][0]\n",
    "#tutaj pasuje sie zastanowic czy nie ma bledu metodologicznego\n",
    "#bo mam watpliwosci czy uzywanie statystyk zbioru testowego dla predykcji jest uzasadnione\n",
    "test = test*(test_min_max[1]-test_min_max[0])+test_min_max[0]\n",
    "\n",
    "plt.plot(np.array(predictions))\n",
    "plt.plot(test[:,0])\n",
    "plt.show()\n",
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
